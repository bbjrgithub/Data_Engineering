# Reddit TCM Movies Data Pipeline Project

Technologies used: *Airflow*, *Docker*, *Python*, *AWS S3 & Redshift*
&nbsp;

This is a re-implementation of Yusuf Ganiyu's (CodeWithYu) [Reddit Data Pipeline Engineering](https://github.com/airscholar/RedditDataEngineering) project. The pipeline extracts data from the ```/r/TurnerClassicMovies``` subreddit, saves it as a .CSV and a Parquet file and loads it into Redshift.

I followed along with the project, typing in the code by hand and changing the names of various variables/functions so that I could understand the flow of the code. I made the following adjustments/additions:

1. Updated the code to use an Airflow 3.1.1 Docker image and Python 3.11.
2. As I am running Fedora I edited the ```docker-compose.yaml``` and added SELinux permissions for the volume mounts.
3. Added support to output the posts to Parquet format.
4. Did not remove any of the parsed fields from the Reddit posts and did not use Glue/Athena to perform any transformations. Instead added code to load the .CSV into Redshift.
5. Changed ```constants.py``` to use ```RawConfigParser``` instead of ```ConfigParser``` so that special characters could be used in the Redshift cluster password without escaping them.

## Prerequisites

- Fedora or other distributive that uses SELinux.
  - If SELinux is not being used the permissions for the volume mounts need to be removed from ```docker-compose.yaml```.
- Docker and Docker Compose.
- Python 3.11.
- An AWS account with appropriate permissions for S3 and Redshift.
- An existing Redshift cluster.
- A [Reddit API key and OAUTH Client ID/App ID](https://www.reddit.com/r/reddit.com/wiki/api/) (Also see [here](https://www.reddit.com/r/redditdev/comments/hasnnc/comment/ijwbc20/))

## Installation

1. [Create](S3_And_Redshift_Setup.md) the S3 bucket and Redshift cluster if they are not already created.
   - Note: The bucket will be created in the region defined in the ```aws_region``` variable in the ```config.conf``` file if it does not already exist.
   - Note: If the bucket and cluster are not in the same region a ```The S3 bucket addressed by the query is in a different region from this cluster``` error is generated by Redshift when it tries to copy the .CSV from the bucket.
2. Clone the repository.
3. Create and activate a virtual environment:

       $ virtualenv -p python3.11 .venv311
       $ source .venv311/bin/activate
4. Install Airflow 3.1.1 with package constraints and the Python dependencies:

       (.venv311) $ pip install "apache-airflow==3.1.1" --constraint "https://raw.githubusercontent.com/apache/airflow/constraints-3.1.1/constraints-3.11.txt" pandas numpy praw apache-airflow-providers-fab configparser connexion flask_appbuilder pyarrow redshift_connector s3fs
5. Rename the ```config.conf.example``` file and input configuration:

       (.venv311) $ mv config/config.conf.example config/config.conf
6. May need to run the following:

       (Allow Airflow to read it's configuration)
       (.venv311) $ chmod a+rw config/airflow.cfg

       (Allow Airflow to write logs)
       (.venv311) $ chmod 777 ./logs
7. Initialize ```airflow.cfg``` with default values:

       (.venv311) $ docker compose run airflow-cli airflow config list
8. Initialize the database (run database migrations and create the first user account)

       (.venv311) $ docker compose up airflow-init
9. Start the containers:

       (.venv311) $ docker-compose up -d
10. Launch the Airflow web UI:

       http://localhost:8080

## Demo

![](Reddit_TCM_Movies_Data_Pipeline_Project__10-31-2025.mp4)