import os
import sys
from datetime import datetime

from airflow import DAG
from airflow.providers.standard.operators.python import PythonOperator

"""
For "sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))"

As Airflow is running in Docker if the DAG is run and
there is no insertion into the root directory to reflect
the root directory there will be an error. This fixes it:

Get absolute path (abspath) of the current file, get the
directory name (the "dags" folder the DAG is in) and get
the directory name of the top level directory of the
"dags" folder

So /Reddit_TCM_Movies_Data_Pipeline_Project/dags/reddit_tcm_movies_dag.py

Comment has to go here or an "Module level import not at top of file" error is generated by Ruff
"""
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# sys.path.insert(0, (os.path.dirname(os.path.abspath(__file__))))

from pipelines.copy_to_redshift_pipeline import copy_to_redshift_pipeline
from pipelines.reddit_tcm_movies_pipeline import reddit_tcm_movies_pipeline
from pipelines.upload_to_s3_pipeline import upload_to_s3_pipeline

default_args = {"owner": "Me", "start_date": datetime(2025, 10, 29)}

file_date = datetime.now().strftime("%Y%m%d")

dag = DAG(
    dag_id="etl_reddit_tcm_movies_pipeline",
    default_args=default_args,
    schedule="@daily",
    catchup=False,  # catchup=False,
    tags=["reddit", "tcm_movies", "etl", "pipeline"],
)

# Extraction from the Reddit TurnerClassicMovies subreddit
extract = PythonOperator(
    task_id="reddit_tcm_movies_extraction",
    python_callable=reddit_tcm_movies_pipeline,
    op_kwargs={
        "csv_file_name": f"reddit{file_date}",
        "parquet_file_name": f"reddit{file_date}",
        "subreddit": "TurnerClassicMovies",
        "time_filter": "month",  # Changed from "day" to get more than one day's worth of data
        "limit": 1000,  # Number of posts per day to import
    },
    dag=dag,
)

# Upload .CSV to S3
upload_to_s3 = PythonOperator(
    task_id="upload_to_s3", python_callable=upload_to_s3_pipeline, dag=dag
)

# Copy .CSV to Redshift
copy_to_redshift_db = PythonOperator(
    task_id="copy_to_redshift_db", python_callable=copy_to_redshift_pipeline, dag=dag
)

extract >> upload_to_s3 >> copy_to_redshift_db
